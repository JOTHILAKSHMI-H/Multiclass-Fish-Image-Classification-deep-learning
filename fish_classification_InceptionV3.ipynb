{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Multiclass Fish Image Classification using Pre-trained models**\n",
        "\n",
        "**InceptionV3**"
      ],
      "metadata": {
        "id": "Vm_1MT7VNx2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4u5K4_0EuPWw",
        "outputId": "f32cf7b5-76a9-43a0-9e50-1e5d051f3e35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uz5AL2LjNeuD"
      },
      "outputs": [],
      "source": [
        "#import required libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, datasets, models\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "from PIL import Image\n",
        "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set the device\n",
        "#This means you’ll run your model on GPU\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "b0x3wccaORZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transformation\n",
        "#InceptionV3 expects images of size 299x299.\n",
        "# for training data\n",
        "\n",
        "train_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.RandomResizedCrop(299, scale = (0.8, 1.0)), #random zoom + crop\n",
        "        transforms.RandomHorizontalFlip(), # flip the image (right ---> left)\n",
        "        transforms.RandomRotation(15), # random rotation clockwise or counter clockwise\n",
        "        transforms.ToTensor() #rescale to [0,1]\n",
        "\n",
        "    ])"
      ],
      "metadata": {
        "id": "1xxG6DlqOUkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for test and val\n",
        "# data preprocessing\n",
        "\n",
        "test_val_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((299, 299)), #resize piexel same as training\n",
        "        transforms.ToTensor() #convert image to tensor\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "NqHboCk6OYao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# load the dataset\n",
        "\n",
        "train_datasets = datasets.ImageFolder('/content/drive/MyDrive/images.cv_jzk6llhf18tm3k0kyttxz/data/train', transform = train_transform)\n",
        "\n",
        "test_datasets = datasets.ImageFolder('/content/drive/MyDrive/images.cv_jzk6llhf18tm3k0kyttxz/data/test', transform= test_val_transform)\n",
        "\n",
        "val_datasets = datasets.ImageFolder('/content/drive/MyDrive/images.cv_jzk6llhf18tm3k0kyttxz/data/val', transform= test_val_transform)\n"
      ],
      "metadata": {
        "id": "XpudXC-DObEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the dataloader\n",
        "train_data_loader = DataLoader(train_datasets, batch_size = 5, shuffle = True)\n",
        "test_data_loader = DataLoader(test_datasets, batch_size = 5, shuffle = True)\n",
        "val_data_loader = DataLoader(val_datasets, batch_size = 5, shuffle = True)\n"
      ],
      "metadata": {
        "id": "XqWEYnxuOhhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the pre-trained  model ResNet50\n",
        "model = models.inception_v3(pretrained = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfBwtcTdPH6f",
        "outputId": "feb253e2-850e-4768-f92e-00024adb2481"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n",
            "100%|██████████| 104M/104M [00:00<00:00, 197MB/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fc = nn.Linear(model.fc.in_features, 11)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "Um2PGKZKSW_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loss function and optimizer\n",
        "\n",
        "cl = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "mvChfqkESiXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "\n",
        "epochs = 25\n",
        "\n",
        "for i in range(epochs):\n",
        "  model.train()\n",
        "  total_loss = 0.0 # total running loss\n",
        "  total = 0 #  #  Total number of images evaluated\n",
        "  correct = 0 #  calculate how many predictions correct\n",
        "\n",
        "  for img, tar in train_data_loader:\n",
        "    img, tar = img.to(device), tar.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(img)\n",
        "    output_main = outputs.logits  ## InceptionV3 return InceptionOutputs which includes two outputs during training\n",
        "    loss = cl(output_main, tar)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "    _, preds = torch.max(output_main, 1)\n",
        "    correct += (preds == tar).sum().item()\n",
        "    total += tar.size(0)\n",
        "\n",
        "  print(f\"Epoch [{i + 1}/{epochs}], loss {total_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "KdqwhIPFS5x6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5737c905-eaeb-4897-e20c-9aabf50fbf13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/25], loss 824.3374\n",
            "Epoch [2/25], loss 721.5575\n",
            "Epoch [3/25], loss 693.1239\n",
            "Epoch [4/25], loss 663.9621\n",
            "Epoch [5/25], loss 676.7073\n",
            "Epoch [6/25], loss 653.0401\n",
            "Epoch [7/25], loss 630.0506\n",
            "Epoch [8/25], loss 637.2027\n",
            "Epoch [9/25], loss 663.4692\n",
            "Epoch [10/25], loss 677.6671\n",
            "Epoch [11/25], loss 663.7045\n",
            "Epoch [12/25], loss 694.9322\n",
            "Epoch [13/25], loss 672.9573\n",
            "Epoch [14/25], loss 694.4688\n",
            "Epoch [15/25], loss 669.0194\n",
            "Epoch [16/25], loss 698.7586\n",
            "Epoch [17/25], loss 677.7195\n",
            "Epoch [18/25], loss 710.9324\n",
            "Epoch [19/25], loss 733.1925\n",
            "Epoch [20/25], loss 703.2789\n",
            "Epoch [21/25], loss 645.4709\n",
            "Epoch [22/25], loss 668.6607\n",
            "Epoch [23/25], loss 685.3732\n",
            "Epoch [24/25], loss 703.9638\n",
            "Epoch [25/25], loss 681.7332\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_tars = []\n",
        "\n",
        "with torch.no_grad():\n",
        "  for img, tar in val_data_loader:\n",
        "    img, tar = img.to(device), tar.to(device)\n",
        "    outputs = model(img)\n",
        "    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "\n",
        "    all_preds.extend(preds.cpu().numpy())\n",
        "    all_tars.extend(tar.cpu().numpy())\n",
        "\n",
        "\n",
        "  #accuracy\n",
        "  accuracy = accuracy_score(all_tars, all_preds)\n",
        "  print(f\"val_accuracy: {accuracy*100 :.2f}%\")\n",
        "\n",
        "  #precision\n",
        "  precision = precision_score(all_tars , all_preds, average = 'macro')\n",
        "  print(f\"val_precision: {precision*100 :.2f}%\")\n",
        "\n",
        "  # recall\n",
        "  recall = recall_score(all_tars , all_preds, average = 'macro')\n",
        "  print(f\"val_recall: {recall*100 :.2f}%\")\n",
        "\n",
        "\n",
        "#f1 score\n",
        "  f1score = f1_score(all_tars, all_preds, average = 'macro')\n",
        "  print(f\"val f1_score: {f1score *100:.2f}%\")\n",
        "\n",
        "# Calculate confusion matrix\n",
        "  cm = confusion_matrix(all_tars, all_preds)\n",
        "  print('confusion matrix')\n",
        "  print(cm)"
      ],
      "metadata": {
        "id": "p-x2z_LyVc5z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87324e90-23f5-480e-acb5-5fe500b946c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val_accuracy: 96.43%\n",
            "val_precision: 96.79%\n",
            "val_recall: 93.89%\n",
            "val f1_score: 94.97%\n",
            "confusion matrix\n",
            "[[186   0   0   0   0   0   0   1   0   0   0]\n",
            " [  3   7   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0 105   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0  80   2   0   1   9   0   0   2]\n",
            " [  0   0   0   0  97   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0  80   0   0   0  10   0]\n",
            " [  0   0   0   1   0   0 110   1   0   1   0]\n",
            " [  0   0   2   0   0   0   0  95   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0 100   0   0]\n",
            " [  0   0   1   0   1   0   0   2   0  97   0]\n",
            " [  0   0   0   0   0   0   0   2   0   0  96]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on test data\n",
        "\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_tars = []\n",
        "\n",
        "with torch.no_grad():\n",
        "  for img, tar in test_data_loader:\n",
        "\n",
        "    img, tar = img.to(device), tar.to(device)\n",
        "\n",
        "    outputs = model(img)\n",
        "    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "    all_preds.extend(preds.cpu().numpy())\n",
        "    all_tars.extend(tar.cpu().numpy())\n",
        "\n",
        "\n",
        "# accuracy\n",
        "  accuracy = accuracy_score(all_tars, all_preds)\n",
        "  print(f\"Test Accuracy: {accuracy *100:.2f}%\")\n",
        "\n",
        "# precision\n",
        "  precision = precision_score(all_tars, all_preds, average = 'macro')\n",
        "  print(f\"Test precision: {precision *100:.2f}%\")\n",
        "\n",
        "#recall\n",
        "  recall = recall_score(all_tars, all_preds, average = 'macro')\n",
        "  print(f\"Test recall: {recall *100:.2f}%\")\n",
        "\n",
        "\n",
        "#f1 score\n",
        "  f1score = f1_score(all_tars, all_preds, average = 'macro')\n",
        "  print(f\"Test f1_score: {f1score *100:.2f}%\")\n",
        "\n",
        "# Calculate confusion matrix\n",
        "  cm = confusion_matrix(all_tars, all_preds)\n",
        "  print('confusion matrix')\n",
        "  print(cm)\n"
      ],
      "metadata": {
        "id": "UPh8U0ihPO_c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "721becc2-bdf3-4e7f-e7a0-0005260e739b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 95.29%\n",
            "Test precision: 93.37%\n",
            "Test recall: 93.52%\n",
            "Test f1_score: 93.20%\n",
            "confusion matrix\n",
            "[[513   4   2   0   0   0   0   1   0   0   0]\n",
            " [  3  10   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0 298   0   0   0   0   0   0   0   0]\n",
            " [  1   0   2 233   8   0  22  36   0   0   3]\n",
            " [  0   0   3   0 283   0   0   0   0   0   0]\n",
            " [  0   0   1   0   3 254   0   0   0  33   0]\n",
            " [  0   0   0   1   0   0 267   4   0   1   0]\n",
            " [  1   0   4   1   0   0   0 319   0   1   1]\n",
            " [  1   0   0   0   0   0   0   0 287   1   0]\n",
            " [  0   0   4   0   0   0   0   3   0 286   0]\n",
            " [  0   0   0   0   0   0   0   5   0   0 287]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the train data\n",
        "\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_tars = []\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "  for img, tar in train_data_loader:\n",
        "    img, tar = img.to(device), tar.to(device)\n",
        "    outputs = model(img)\n",
        "    _,preds = torch.max(outputs, 1)\n",
        "\n",
        "    all_preds.extend(preds.cpu().numpy())\n",
        "    all_tars.extend(tar.cpu().numpy())\n",
        "\n",
        "\n",
        "  # accuracy\n",
        "  accuracy = accuracy_score(all_tars, all_preds)\n",
        "  print(f\"Train Accuracy: {accuracy* 100:.2f}%\")\n",
        "\n",
        "  #precision\n",
        "  precision = precision_score(all_tars, all_preds, average = 'macro')\n",
        "  print(f\"Train precision: {precision*100:.2f}%\")\n",
        "\n",
        "  #recall\n",
        "  recall = recall_score(all_tars, all_preds, average = 'macro')\n",
        "  print(f\"Train recall:{recall*100:.2f}%\")\n",
        "\n",
        "\n",
        "  #f1 score\n",
        "  f1score = f1_score(all_tars, all_preds, average = 'macro')\n",
        "  print(f\" f1_score: {f1score *100:.2f}%\")\n",
        "\n",
        "  # Calculate confusion matrix\n",
        "  cm = confusion_matrix(all_tars, all_preds)\n",
        "  print('confusion matrix')\n",
        "  print(cm)"
      ],
      "metadata": {
        "id": "tE5o8ZprVpFh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48bee355-3fc0-4642-c6c2-322811ee51af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 97.98%\n",
            "Train precision: 97.18%\n",
            "Train recall:97.18%\n",
            " f1_score: 97.16%\n",
            "confusion matrix\n",
            "[[1087    3    2    0    0    1    0    2    0    1    0]\n",
            " [   3   27    0    0    0    0    0    0    0    0    0]\n",
            " [   0    0  569    0    0    0    0    0    0    0    0]\n",
            " [   0    0    2  517    2    0    7   30    0    0    8]\n",
            " [   1    0    6    0  563    1    0    0    0    2    0]\n",
            " [   0    0    2    0    1  557    0    0    1   18    0]\n",
            " [   1    0    0    5    0    0  562    2    0    1    0]\n",
            " [   1    0    5    1    0    0    0  526    0    1    4]\n",
            " [   0    0    0    0    0    0    0    0  576    0    0]\n",
            " [   0    0    7    0    0    2    0    0    0  538    0]\n",
            " [   1    0    0    0    0    0    0    2    0    0  577]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model\n",
        "\n",
        "torch.save(model.state_dict(), 'inceptionv3_fish_model.pth')"
      ],
      "metadata": {
        "id": "-z5xGTaxVyVV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}